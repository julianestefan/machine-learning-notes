# MLOps: Model Monitoring and Maintenance

Model monitoring is a critical aspect of MLOps that ensures deployed models continue to perform effectively in production environments. This document outlines key monitoring approaches, retraining strategies, maturity models, and relevant tools.

## Monitoring Approaches

### Statistical Monitoring

Statistical monitoring focuses on tracking the statistical properties of model inputs, outputs, and performance metrics to detect anomalies and drift.

#### Key Metrics to Track:

1. **Input Feature Distribution**
   - Feature means, medians, and standard deviations
   - Feature correlations
   - Missing value frequencies

2. **Output Distribution**
   - Prediction class distribution (classification)
   - Prediction value distribution (regression)
   - Prediction confidence scores

3. **Performance Metrics**
   - Accuracy, precision, recall, F1-score
   - AUC-ROC, AUC-PR
   - Mean squared error, mean absolute error

#### Implementation Example:

```python
# statistics_monitor.py
import pandas as pd
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

class DistributionMonitor:
    def __init__(self, reference_data, feature_columns, categorical_columns=None):
        """
        Initialize distribution monitor with reference data.
        
        Args:
            reference_data (DataFrame): Reference data used for training
            feature_columns (list): List of feature column names to monitor
            categorical_columns (list): List of categorical column names
        """
        self.reference_data = reference_data
        self.feature_columns = feature_columns
        self.categorical_columns = categorical_columns or []
        self.numerical_columns = [col for col in feature_columns if col not in self.categorical_columns]
        
        # Compute reference statistics
        self.reference_stats = self._compute_statistics(reference_data)
        
    def _compute_statistics(self, data):
        """Compute basic statistics for each feature"""
        stats_dict = {}
        
        # Numerical features
        for col in self.numerical_columns:
            stats_dict[col] = {
                'mean': data[col].mean(),
                'median': data[col].median(),
                'std': data[col].std(),
                'min': data[col].min(),
                'max': data[col].max(),
                'kurtosis': stats.kurtosis(data[col].dropna()),
                'skewness': stats.skew(data[col].dropna())
            }
            
        # Categorical features
        for col in self.categorical_columns:
            stats_dict[col] = {
                'distribution': data[col].value_counts(normalize=True).to_dict(),
                'unique_count': data[col].nunique()
            }
            
        return stats_dict
    
    def detect_drift(self, new_data, threshold=0.05):
        """
        Detect statistical drift in new data compared to reference data.
        
        Args:
            new_data (DataFrame): New data to check for drift
            threshold (float): p-value threshold for KS test
            
        Returns:
            dict: Dictionary of drift detection results
        """
        results = {}
        
        # Check numerical columns with Kolmogorov-Smirnov test
        for col in self.numerical_columns:
            # Skip if column doesn't exist in new data
            if col not in new_data.columns:
                results[col] = {'drift_detected': True, 'reason': 'Column missing'}
                continue
                
            ref_data = self.reference_data[col].dropna()
            new_col_data = new_data[col].dropna()
            
            if len(new_col_data) < 10:
                results[col] = {'drift_detected': True, 'reason': 'Insufficient data'}
                continue
                
            # Perform KS test
            ks_stat, p_value = stats.ks_2samp(ref_data, new_col_data)
            drift_detected = p_value < threshold
            
            # Calculate basic statistics
            new_stats = {
                'mean': new_col_data.mean(),
                'median': new_col_data.median(),
                'std': new_col_data.std(),
                'min': new_col_data.min(),
                'max': new_col_data.max(),
                'ks_stat': ks_stat,
                'p_value': p_value,
                'drift_detected': drift_detected
            }
            
            results[col] = new_stats
            
        # Check categorical columns with chi-squared test
        for col in self.categorical_columns:
            if col not in new_data.columns:
                results[col] = {'drift_detected': True, 'reason': 'Column missing'}
                continue
                
            # Get distributions
            ref_dist = pd.Series(self.reference_stats[col]['distribution'])
            new_dist = new_data[col].value_counts(normalize=True)
            
            # Align distributions (fill missing categories with 0)
            aligned_dists = pd.concat([ref_dist, new_dist], axis=1, sort=True).fillna(0)
            aligned_dists.columns = ['reference', 'new']
            
            # Chi-squared test
            chi2_stat, p_value = stats.chisquare(
                aligned_dists['new'] * 100,  # Convert to count-like numbers
                aligned_dists['reference'] * 100
            )
            
            drift_detected = p_value < threshold
            
            results[col] = {
                'unique_values_reference': self.reference_stats[col]['unique_count'],
                'unique_values_new': new_data[col].nunique(),
                'chi2_stat': chi2_stat,
                'p_value': p_value,
                'drift_detected': drift_detected
            }
            
        return results
    
    def generate_drift_report(self, new_data, output_path=None):
        """Generate comprehensive drift report with visualizations"""
        drift_results = self.detect_drift(new_data)
        
        # Create report dataframe
        report_data = []
        for feature, result in drift_results.items():
            report_data.append({
                'Feature': feature,
                'Drift_Detected': result.get('drift_detected', True),
                'p_value': result.get('p_value', 0),
                'Test_Type': 'KS test' if feature in self.numerical_columns else 'Chi-squared test'
            })
            
        report_df = pd.DataFrame(report_data)
        
        # Create visualizations
        fig, axes = plt.subplots(len(self.numerical_columns), 1, figsize=(10, 4 * len(self.numerical_columns)))
        
        for i, col in enumerate(self.numerical_columns):
            if isinstance(axes, np.ndarray):
                ax = axes[i]
            else:
                ax = axes
            
            # Plot distributions
            sns.kdeplot(self.reference_data[col].dropna(), ax=ax, label='Reference')
            sns.kdeplot(new_data[col].dropna(), ax=ax, label='New Data')
            ax.set_title(f"{col} Distribution Comparison")
            ax.legend()
            
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = output_path or f"drift_report_{timestamp}.pdf"
        plt.tight_layout()
        fig.savefig(report_path)
        
        return report_df, report_path
```

### Computational Monitoring

Computational monitoring focuses on the operational aspects of model serving, such as response latency, throughput, resource utilization, and service availability.

#### Key Metrics to Track:

1. **Performance Metrics**
   - Inference latency (p50, p95, p99 percentiles)
   - Throughput (requests per second)
   - Batch size optimization

2. **Resource Utilization**
   - CPU usage
   - Memory consumption
   - GPU utilization
   - Disk I/O

3. **Service Reliability**
   - Error rates
   - Service availability
   - Recovery time

#### Prometheus Configuration Example:

```yaml
# prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'model_service'
    metrics_path: '/metrics'
    static_configs:
      - targets: ['model-service:8000']
```

#### Custom Metrics in FastAPI:

```python
# app.py
import time
from fastapi import FastAPI, Request
from prometheus_client import Counter, Histogram, Gauge, generate_latest
from prometheus_client import CONTENT_TYPE_LATEST

app = FastAPI()

# Define Prometheus metrics
REQUESTS = Counter('model_requests_total', 'Total model requests', ['model', 'endpoint'])
LATENCY = Histogram('model_request_latency_seconds', 'Request latency in seconds',
                   ['model', 'endpoint'], buckets=(0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0))
PREDICTIONS = Counter('model_predictions_total', 'Predictions by class', 
                      ['model', 'class'])
ERRORS = Counter('model_errors_total', 'Errors during prediction', 
                 ['model', 'error_type'])
FEATURES_GAUGE = Gauge('model_feature_value', 'Feature values', 
                       ['model', 'feature'])

# Middleware to track request latency
@app.middleware("http")
async def add_metrics(request: Request, call_next):
    start_time = time.time()
    response = await call_next(request)
    process_time = time.time() - start_time
    
    if request.url.path in ['/predict', '/batch_predict']:
        REQUESTS.labels('credit_risk_model', request.url.path).inc()
        LATENCY.labels('credit_risk_model', request.url.path).observe(process_time)
    
    return response

# Metrics endpoint for Prometheus to scrape
@app.get('/metrics')
def metrics():
    return Response(content=generate_latest(), media_type=CONTENT_TYPE_LATEST)

# Regular prediction endpoint
@app.post('/predict')
async def predict(features: dict):
    try:
        # Log feature values for distribution tracking
        for feature, value in features.items():
            if isinstance(value, (int, float)):
                FEATURES_GAUGE.labels('credit_risk_model', feature).set(value)
        
        # Make prediction
        result = model.predict([features])
        prediction = result[0]
        
        # Increment prediction counter by class
        PREDICTIONS.labels('credit_risk_model', str(prediction)).inc()
        
        return {"prediction": prediction}
    except Exception as e:
        # Track errors
        ERRORS.labels('credit_risk_model', type(e).__name__).inc()
        raise
```

### Feedback Loop Monitoring

Feedback loop monitoring involves comparing model predictions with actual ground truth labels that become available after prediction, allowing for the measurement of real-world model performance.

#### Implementation Example:

```python
# feedback_monitor.py
import pandas as pd
import numpy as np
from sklearn.metrics import confusion_matrix, precision_recall_fscore_support
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta

class FeedbackMonitor:
    def __init__(self, model_id, window_size=30):
        """
        Initialize feedback monitor.
        
        Args:
            model_id (str): Identifier for the model
            window_size (int): Size of sliding window in days
        """
        self.model_id = model_id
        self.window_size = window_size
        self.predictions_db = []  # In production, this would be a database
        
    def log_prediction(self, prediction_id, features, prediction, confidence=None, metadata=None):
        """Log a new prediction"""
        self.predictions_db.append({
            'prediction_id': prediction_id,
            'model_id': self.model_id,
            'timestamp': datetime.now(),
            'features': features,
            'prediction': prediction,
            'confidence': confidence,
            'ground_truth': None,  # Will be updated later
            'metadata': metadata or {}
        })
        
    def log_ground_truth(self, prediction_id, ground_truth, feedback_metadata=None):
        """Log ground truth for a previous prediction"""
        for i, pred in enumerate(self.predictions_db):
            if pred['prediction_id'] == prediction_id:
                self.predictions_db[i]['ground_truth'] = ground_truth
                self.predictions_db[i]['feedback_timestamp'] = datetime.now()
                self.predictions_db[i]['feedback_metadata'] = feedback_metadata or {}
                break
                
    def get_performance_metrics(self, start_date=None, end_date=None):
        """Calculate performance metrics for predictions with ground truth"""
        # Filter by date range if specified
        if start_date is None:
            start_date = datetime.now() - timedelta(days=self.window_size)
        if end_date is None:
            end_date = datetime.now()
            
        # Get predictions with ground truth in date range
        filtered_preds = [
            p for p in self.predictions_db 
            if p['ground_truth'] is not None 
            and start_date <= p['timestamp'] <= end_date
        ]
        
        if not filtered_preds:
            return {"error": "No ground truth data available in specified range"}
            
        # Extract predictions and ground truth
        y_pred = [p['prediction'] for p in filtered_preds]
        y_true = [p['ground_truth'] for p in filtered_preds]
        
        # Calculate metrics
        precision, recall, f1, _ = precision_recall_fscore_support(
            y_true, y_pred, average='weighted')
        
        # Create confusion matrix
        labels = sorted(list(set(y_true + y_pred)))
        cm = confusion_matrix(y_true, y_pred, labels=labels)
        
        return {
            'model_id': self.model_id,
            'start_date': start_date,
            'end_date': end_date,
            'sample_count': len(filtered_preds),
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'confusion_matrix': {
                'matrix': cm.tolist(),
                'labels': labels
            }
        }
        
    def generate_performance_report(self, output_path=None):
        """Generate visual performance report"""
        metrics = self.get_performance_metrics()
        
        if 'error' in metrics:
            return metrics
            
        # Create confusion matrix heatmap
        plt.figure(figsize=(10, 8))
        cm = np.array(metrics['confusion_matrix']['matrix'])
        labels = metrics['confusion_matrix']['labels']
        
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=labels, yticklabels=labels)
        plt.xlabel('Predicted')
        plt.ylabel('Actual')
        plt.title(f'Confusion Matrix - Model {self.model_id}')
        
        # Save or show
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = output_path or f"performance_report_{self.model_id}_{timestamp}.pdf"
        plt.savefig(report_path)
        
        return {
            **metrics,
            'report_path': report_path
        }
```

### Performance Comparison Table

| Monitoring Type | Primary Focus | Advantages | Challenges | When to Use |
|-----------------|---------------|------------|------------|-------------|
| **Statistical** | Input/output distributions | Early drift detection; No ground truth needed | May detect changes that don't affect performance | Always; First line of monitoring |
| **Computational** | Resource usage; Response time | Ensures operational reliability | Doesn't detect data or concept drift | Always; Critical for SLAs |
| **Feedback Loop** | Actual vs. predicted | Most accurate performance evaluation | Requires ground truth data; Often delayed | When ground truth is available |

## Retraining Strategies

Knowing when and how to retrain models is crucial for maintaining performance over time. Two primary drift types necessitate retraining:

### Data Drift

Data drift occurs when the statistical properties of the input data change over time, making the model's learned patterns less relevant.

#### Common Causes:
- Seasonal changes in user behavior
- Market shifts affecting customer preferences
- Changes in data collection methods
- Introduction of new features or products

#### Detection Methods:
- Statistical tests (Kolmogorov-Smirnov, Chi-squared)
- Population Stability Index (PSI)
- Jensen-Shannon divergence

```python
# data_drift.py
import pandas as pd
import numpy as np
from scipy import stats

def calculate_psi(expected, actual, buckettype='bins', buckets=10):
    """
    Calculate Population Stability Index (PSI)
    
    Args:
        expected: Reference distribution (training data)
        actual: Current distribution
        buckettype: 'bins' for equal-width bins, 'quantiles' for equal-size bins
        buckets: Number of buckets to use
        
    Returns:
        PSI value (less than 0.1 is minimal drift, 0.1-0.25 is moderate, above 0.25 is significant)
    """
    # Remove NULL values
    expected = expected[~pd.isnull(expected)]
    actual = actual[~pd.isnull(actual)]
    
    # Create buckets
    if buckettype == 'bins':
        breakpoints = np.arange(0, buckets + 1) / buckets
        breakpoints = stats.scoreatpercentile(
            np.append(expected, actual), breakpoints * 100
        )
    elif buckettype == 'quantiles':
        breakpoints = np.percentile(expected, np.arange(0, buckets + 1) / buckets * 100)
    else:
        raise ValueError("Bucket type not recognized")
    
    # Ensure unique breakpoints
    breakpoints = np.unique(breakpoints)
    if len(breakpoints) <= 1:
        return 0  # No variation, so no drift
    
    # Calculate frequencies in buckets
    expected_counts = np.histogram(expected, breakpoints)[0] + 0.0001  # Avoid zeros
    actual_counts = np.histogram(actual, breakpoints)[0] + 0.0001
    
    # Normalize to get percentages
    expected_percents = expected_counts / sum(expected_counts)
    actual_percents = actual_counts / sum(actual_counts)
    
    # Calculate PSI
    psi_value = sum((actual_percents - expected_percents) * 
                    np.log(actual_percents / expected_percents))
    
    return psi_value
```

### Concept Drift

Concept drift occurs when the relationships between input features and the target variable change, requiring updated model understanding.

#### Common Causes:
- Changes in customer behavior patterns
- Regulatory or policy changes affecting decision-making
- New competitor offerings changing market dynamics
- Economic shifts altering risk factors

#### Detection Methods:
- Performance degradation monitoring
- Model error analysis over time
- Adversarial validation

```python
# concept_drift.py
import pandas as pd
import numpy as np
from sklearn.metrics import roc_auc_score
from sklearn.ensemble import RandomForestClassifier

def detect_concept_drift(reference_X, reference_y, current_X, current_y):
    """
    Detect concept drift by comparing model performance on reference vs. current data
    
    Returns:
        Drift metrics including AUC drop and confidence score
    """
    # Train model on reference data
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(reference_X, reference_y)
    
    # Evaluate on both datasets
    reference_pred = model.predict_proba(reference_X)[:, 1]
    current_pred = model.predict_proba(current_X)[:, 1]
    
    ref_auc = roc_auc_score(reference_y, reference_pred)
    cur_auc = roc_auc_score(current_y, current_pred)
    
    # Calculate AUC drop
    auc_drop = ref_auc - cur_auc
    
    # Perform adversarial validation
    # Create a dataset where 0 = reference data, 1 = current data
    combined_X = pd.concat([reference_X, current_X])
    combined_y = np.array([0] * len(reference_X) + [1] * len(current_X))
    
    # Train classifier to distinguish between reference and current data
    adv_model = RandomForestClassifier(n_estimators=100, random_state=42)
    adv_model.fit(combined_X, combined_y)
    
    # Get adversarial AUC - closer to 0.5 means distributions are similar
    adversarial_pred = adv_model.predict_proba(combined_X)[:, 1]
    adv_auc = roc_auc_score(combined_y, adversarial_pred)
    
    # Calculate drift metrics
    drift_score = (adv_auc - 0.5) * 2  # Scale to 0-1, where 0 = no drift
    
    return {
        'reference_auc': ref_auc,
        'current_auc': cur_auc, 
        'auc_drop': auc_drop,
        'adversarial_auc': adv_auc,
        'drift_score': drift_score,
        'significant_drift': drift_score > 0.3 or auc_drop > 0.05
    }
```

### Retraining Frequency Decision Factors

Determining optimal retraining frequency depends on multiple factors:

#### Business Environment
- Criticality of predictions
- Cost of incorrect predictions
- Pace of environmental change
- Regulatory requirements

#### Technical Factors
- Data collection volume and frequency
- Computational resources available
- Pipeline automation level
- Model complexity

#### Cost-Benefit Analysis
- Retraining costs (computing, validation, deployment)
- Performance improvement benefits
- Operational disruption risks

#### Decision Framework:

| Retraining Trigger | Description | Example Use Case |
|-------------------|-------------|-----------------|
| **Schedule-based** | Fixed intervals (daily, weekly, monthly) | Regular, predictable environments |
| **Performance-based** | When metrics drop below threshold | Critical applications with clear KPIs |
| **Data volume-based** | After collecting X new samples | Sparse data environments |
| **Drift-based** | When drift metrics exceed thresholds | Dynamic environments |
| **Hybrid** | Combination of triggers | Most production systems |

## MLOps Maturity Model

Organizations typically evolve through several maturity levels in their MLOps practices:

### Level 1: Manual Process
- Manual experimentation, training, and deployment
- Limited monitoring and documentation
- No version control for data or models
- No automation

### Level 2: Partial Automation
- Automated training and evaluation
- Basic CI/CD for model deployment
- Manual monitoring and intervention
- Basic version control for code and models
- Limited reproducibility

### Level 3: Full MLOps
- End-to-end automation of ML lifecycle
- Comprehensive monitoring and alerting
- Automated retraining triggered by drift or performance
- Complete version control for code, data, and models
- Fully reproducible pipelines
- Governance and compliance integration

#### Maturity Comparison Table

| Aspect | Level 1 | Level 2 | Level 3 |
|--------|---------|---------|---------|
| **Experimentation** | Ad-hoc notebooks | Tracked experiments | Systematic experimentation |
| **Development** | Manual scripts | Versioned code repositories | Modular, tested ML components |
| **Deployment** | Manual deployment | Semi-automated deployment | Automated CI/CD pipelines |
| **Monitoring** | Manual checks | Basic automated monitoring | Comprehensive monitoring & alerting |
| **Governance** | Limited documentation | Basic model cards | Full lineage tracking & approval workflows |
| **Retraining** | Manual retraining | Scheduled retraining | Drift-triggered retraining |

## MLOps Tool Ecosystem

### Feature Stores

Feature stores manage the creation, storage, and serving of features for machine learning models.

| Tool | Type | Key Features | Best For |
|------|------|-------------|----------|
| **Feast** | Self-managed, open source | Simple setup, offline/online stores, feature registry | Teams starting with feature management |
| **Hopsworks** | Open source/enterprise | End-to-end platform, Feature Store, Model Registry | Organizations needing comprehensive platform |
| **Tecton** | Commercial cloud | Production-grade, real-time features, monitoring | Enterprise-scale real-time ML |
| **Amazon SageMaker FS** | Cloud-native | Integrated with AWS ecosystem | AWS-based ML workflows |

### Experiment Tracking

Tools for tracking model training runs, parameters, metrics, and artifacts.

| Tool | Focus | Integration | Visualization | Best For |
|------|-------|-------------|--------------|----------|
| **MLflow** | General purpose | Many frameworks | Basic dashboards | Teams needing open-source flexibility |
| **ClearML** | Full MLOps | Good orchestration | Advanced UI | Organizations scaling MLOps practices |
| **Weights & Biases** | Experiment tracking | Deep learning focus | Advanced visualizations | Deep learning and computer vision teams |
| **Neptune.ai** | Metadata store | Flexible logging | Custom dashboards | Teams needing detailed experiment tracking |

#### MLflow Configuration Example:

```python
# mlflow_tracking.py
import mlflow
import mlflow.sklearn
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Set tracking URI - local or remote server
mlflow.set_tracking_uri("http://localhost:5000")
mlflow.set_experiment("credit_risk_model")

# Load and prepare data
X_train, X_test, y_train, y_test = load_data()

# Start an MLflow run
with mlflow.start_run(run_name="random_forest_model"):
    # Set parameters
    n_estimators = 100
    max_depth = 10
    
    # Log parameters
    mlflow.log_param("n_estimators", n_estimators)
    mlflow.log_param("max_depth", max_depth)
    mlflow.log_param("data_version", "v1.2")
    
    # Train model
    model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth)
    model.fit(X_train, y_train)
    
    # Make predictions and evaluate
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    
    # Log metrics
    mlflow.log_metric("accuracy", accuracy)
    
    # Log feature importance
    for idx, importance in enumerate(model.feature_importances_):
        mlflow.log_metric(f"feature_importance_{X_train.columns[idx]}", importance)
    
    # Log model
    mlflow.sklearn.log_model(model, "model", registered_model_name="credit_risk_model")
    
    # Log artifacts
    mlflow.log_artifact("data_description.txt")
    
    # Set tags for better organization
    mlflow.set_tag("model_type", "random_forest")
    mlflow.set_tag("developer", "data_science_team")
```

### Containerization

Tools for packaging ML models and dependencies into portable containers.

| Tool | Focus | Orchestration | Best For |
|------|-------|--------------|----------|
| **Docker** | Container creation | Basic | Individual containers |
| **Kubernetes** | Container orchestration | Advanced | Scaling and managing containers |
| **RedHat OpenShift** | Enterprise K8s | Advanced + UI | Enterprise container management |
| **Google Cloud Run** | Serverless containers | Managed | Lightweight container deployment |

### Monitoring Tools

Specialized tools for monitoring ML model performance in production.

| Tool | Focus | Key Features | Integration |
|------|-------|-------------|-------------|
| **Fiddler AI** | Explainability | Root cause analysis, custom metrics | Many ML frameworks |
| **Great Expectations** | Data validation | Schema validation, data quality | Python ecosystem |
| **Seldon Core** | Model serving | Advanced monitoring, A/B testing | Kubernetes |
| **Evidently AI** | Model monitoring | Drift detection, visual reports | Open source, Python |
| **Prometheus/Grafana** | Metrics & visualization | Time-series, alerting | Language-agnostic |

#### Great Expectations Example:

```python
# great_expectations_validation.py
import great_expectations as ge
import pandas as pd
from datetime import datetime

# Load data to validate
input_data = pd.read_csv("incoming_data.csv")

# Convert to Great Expectations DataFrame
ge_df = ge.from_pandas(input_data)

# Define expectations
validation_results = ge_df.expect_column_values_to_not_be_null("customer_id")
validation_results &= ge_df.expect_column_values_to_be_between("age", min_value=18, max_value=120)
validation_results &= ge_df.expect_column_values_to_be_in_set("loan_status", ["approved", "rejected", "pending"])
validation_results &= ge_df.expect_column_values_to_match_regex("email", r"[^@]+@[^@]+\.[^@]+")
validation_results &= ge_df.expect_column_mean_to_be_between("loan_amount", min_value=5000, max_value=50000)

# Get validation results
results = validation_results.to_json_dict()

# Save validation results
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
with open(f"validation_results_{timestamp}.json", "w") as f:
    json.dump(results, f)

# Check if validation passed
if validation_results.success:
    print("Data validation passed! Proceeding with prediction.")
    # Continue with model prediction
else:
    print("Data validation failed! Check validation results for details.")
    # Log error and handle invalid data
```

### Cloud MLOps Platforms

Integrated platforms provided by major cloud providers.

| Platform | Provider | Key Features | Best For |
|----------|----------|-------------|----------|
| **AWS SageMaker** | Amazon | End-to-end ML, AutoML, distributed training | AWS-native organizations |
| **Azure ML** | Microsoft | Integration with Azure, MLOps automation | Microsoft-stack teams |
| **Google Vertex AI** | Google | Unified ML platform, AutoML, end-to-end | Google Cloud users |
| **Databricks** | Multi-cloud | Unified analytics, MLflow integration | Big data + ML workflows |

## Conclusion

Effective model monitoring and maintenance are essential components of MLOps that ensure machine learning systems continue to deliver value over time. By implementing comprehensive monitoring strategies, establishing appropriate retraining triggers, and leveraging the right tools, organizations can build robust ML systems that adapt to changing conditions and maintain performance in production environments.
